{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sitcoms 3: Classification with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the third of four notebooks completed on Sitcom Pilot Script Data. In this notebook, I will model the sitcom pilot data using classification techniques specific to Natural Language Processing. For those of you just joining us, this is the data that we obtained and cleaned in [Notebook 1 - Obtain/Clean](1_sitcoms_clean.ipynb) and explored in [Notebook 2 - EDA](2_sitcoms_EDA.ipynb). After I finish preprocessing the data and evaluating classifier performance, we will be ready for the grand finale - [Notebook 4 - Deep Learning](4_sitcoms_deep_learning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "  * 1. [Import Libraries](#import)\n",
    "      * 1.1 [Open and Read Data](#open)\n",
    "  * 2. [Text Data Preprocessing](#text)  \n",
    "      * 2.1. [Converting Scripts to Lower Case](#lower)  \n",
    "      * 2.2. [Removing Punctuation from Script](#punct)  \n",
    "      * 2.3. [Removing StopWords](#stop)  \n",
    "      * 2.4  [Common Word Removal](#common)  \n",
    "      * 2.5  [Tokenization](#token)  \n",
    "  * 3. [TF-IDF](#tfidf)  \n",
    "      * 3.1. [Frequency Distributions](#freqdist)  \n",
    "      * 3.2. [TF-IDF Vectorization](#vector)  \n",
    "      * 3.3. [Classifiers](#class)  \n",
    "      * 3.4. [TF-IDF Performance](#tfperm)  \n",
    "  * 4. [Latent Semantic Analysis](#lsa)  \n",
    "      * 4.1. [LSA Performance](#lsaperm)   \n",
    "  * 5. [Combining TF-IDF with Descriptive Features](#combo)  \n",
    "      * 5.1. [TF-IDF/Feature Combination Performance](#comboperm)  \n",
    "      * 5.2. [Scaling Numerical Data](#scale)\n",
    "      * 5.3. [SciPy Hstack](#combine)\n",
    "  * 6.  [Classification on Descriptive Feature Set](#feature)  \n",
    "      * 6.1. [Descriptive Feature Performance ](#featperm)  \n",
    "  * 7.  [Evaluation of Models](#eval)\n",
    "      * 7.1. [Test Scores](#test)\n",
    "      * 7.2. [Train Scores](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our data is prepared for modeling! Well, almost. We still have a bit of pre-processing to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "#dataframes\n",
    "import pandas as pd\n",
    "\n",
    "#math\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "#NLP\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#visualizations\n",
    "import seaborn as sns\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# machine learning\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='open'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Open and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = pd.read_csv('scripts.csv') #open and read clean script df\n",
    "df_clean = pd.read_csv('pilotfeatures.csv') #open and read features script df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='text'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lower'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Converting Scripts to Lower Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to replace all instances of upper case letters with their lower case counter-parts. This way, we will not have multiple copies of the same word (i.e., \"Discombobulation\" and \"discombobulation\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['script'] = df_clean['script'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='punct'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Removing Punctuation from Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already extracted variables from our punctuation, so the next step in cleaning is to remove punctuation as it will make our text easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['script'] = df_clean['script'].str.replace('[^\\w\\s]','') #replacing punctuation with spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stop'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Removing StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using NLTK's predefined library of stopwords (super common words such as \"a\" or \"on\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english') #define stopwords through NLTK's library\n",
    "\n",
    "#replacing stopwords with space\n",
    "df_clean['script'] = df_clean['script'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='common'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Common Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the 10 most frequently occurring words in the script lines and then make a call to remove these words or retain them in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "im       1644\n",
       "know     1224\n",
       "oh       1102\n",
       "dont     1016\n",
       "like      994\n",
       "yeah      869\n",
       "youre     846\n",
       "get       836\n",
       "okay      835\n",
       "right     773\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most common words\n",
    "freq = pd.Series(' '.join(df_clean['script']).split()).value_counts()[:10] \n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can definitely make an argument to leave these words in, but I think that for classification purposes, we are better off without them. They do not seem particularly informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list of most common words\n",
    "freq = list(freq.index)\n",
    "#removing them from script\n",
    "df_clean['script'] = df_clean['script'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to go ahead and save our data as is because we may use a different type of tokenizer in our deep learning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('cleandata.csv') #saving the file to access it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='token'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data also needs to be tokenized, meaning that we need to divide the text into a sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#apply nltk's word tokenizer to our data\n",
    "df_clean.script = df_clean['script'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tfidf'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand TF-IDF, we should go back a step. Term frequency (which is the TF in TF-IDF) is the ratio of the count of a word present in a sentence to the length of said sentence.\n",
    "\n",
    "In other words:\n",
    "\n",
    "TF = (Number of times term T appears in the particular row) / (number of terms in that row)\n",
    "\n",
    "The second part is IDF (Inverse Document Frequency), which can be defined as the log of the ratio of the total number of rows to the number of rows in which said word exists. We use IDF because a word isn't very informative to us if it’s appearing in each of the documents.\n",
    "\n",
    "IDF = log(N/n), where N is the total number of rows and n is the number of rows in which the word was present\n",
    "\n",
    "Finally, TF-IDF is the multiplication of the TF and IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='freqdist'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Frequency Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue to explore our data by using frequency distributions to see which terms pop up the most in our scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10810"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's start with getting a list of all of the unique words in our data\n",
    "#since it's a \"set\" it will only gather unique words, no duplicates\n",
    "total_vocab = set()\n",
    "\n",
    "for line in df_clean.script:\n",
    "    total_vocab.update(line)\n",
    "len(total_vocab) #let's see how long this list of unique words is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have almost 11,000 unique words in our total vocabulary for this data! Now let's gather each line into a single list and use FreqDist() to examine the frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate each line of each script into a single list\n",
    "lines_concat = []\n",
    "for line in df_clean.script:\n",
    "    lines_concat += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('na', 734),\n",
       " ('got', 682),\n",
       " ('thats', 680),\n",
       " ('hey', 675),\n",
       " ('go', 670),\n",
       " ('gon', 602),\n",
       " ('well', 585),\n",
       " ('good', 521),\n",
       " ('one', 497),\n",
       " ('think', 473),\n",
       " ('want', 465),\n",
       " ('come', 425),\n",
       " ('really', 423),\n",
       " ('look', 414),\n",
       " ('see', 365),\n",
       " ('man', 354),\n",
       " ('time', 326),\n",
       " ('back', 321),\n",
       " ('going', 320),\n",
       " ('little', 315),\n",
       " ('say', 312),\n",
       " ('hes', 311),\n",
       " ('yes', 304),\n",
       " ('cant', 303),\n",
       " ('ill', 295),\n",
       " ('mean', 286),\n",
       " ('would', 285),\n",
       " ('uh', 280),\n",
       " ('need', 280),\n",
       " ('tell', 275),\n",
       " ('didnt', 267),\n",
       " ('love', 261),\n",
       " ('sorry', 255),\n",
       " ('guys', 252),\n",
       " ('people', 245),\n",
       " ('let', 241),\n",
       " ('take', 240),\n",
       " ('something', 237),\n",
       " ('make', 237),\n",
       " ('whats', 229),\n",
       " ('could', 228),\n",
       " ('god', 226),\n",
       " ('us', 225),\n",
       " ('never', 223),\n",
       " ('thing', 219),\n",
       " ('ive', 216),\n",
       " ('lets', 213),\n",
       " ('theres', 208),\n",
       " ('thank', 207),\n",
       " ('two', 206),\n",
       " ('dad', 206),\n",
       " ('great', 201),\n",
       " ('way', 199),\n",
       " ('even', 197),\n",
       " ('said', 191),\n",
       " ('mom', 189),\n",
       " ('wait', 189),\n",
       " ('work', 187),\n",
       " ('shes', 184),\n",
       " ('first', 179),\n",
       " ('sure', 170),\n",
       " ('call', 168),\n",
       " ('maybe', 161),\n",
       " ('guy', 159),\n",
       " ('still', 158),\n",
       " ('new', 156),\n",
       " ('hi', 155),\n",
       " ('talk', 154),\n",
       " ('night', 152),\n",
       " ('put', 151),\n",
       " ('much', 150),\n",
       " ('nice', 150),\n",
       " ('day', 149),\n",
       " ('please', 148),\n",
       " ('kids', 148),\n",
       " ('big', 147),\n",
       " ('theyre', 146),\n",
       " ('give', 144),\n",
       " ('cause', 143),\n",
       " ('thought', 141),\n",
       " ('life', 140),\n",
       " ('um', 139),\n",
       " ('feel', 138),\n",
       " ('school', 138),\n",
       " ('actually', 134),\n",
       " ('stop', 134),\n",
       " ('boy', 134),\n",
       " ('show', 133),\n",
       " ('always', 132),\n",
       " ('girl', 132),\n",
       " ('anything', 131),\n",
       " ('fine', 129),\n",
       " ('car', 128),\n",
       " ('bad', 127),\n",
       " ('job', 125),\n",
       " ('better', 123),\n",
       " ('around', 123),\n",
       " ('ever', 121),\n",
       " ('wan', 120),\n",
       " ('things', 118),\n",
       " ('talking', 116),\n",
       " ('home', 116),\n",
       " ('best', 116),\n",
       " ('help', 114),\n",
       " ('hello', 114),\n",
       " ('lot', 114),\n",
       " ('keep', 114),\n",
       " ('woman', 113),\n",
       " ('whoa', 112),\n",
       " ('kind', 111),\n",
       " ('getting', 110),\n",
       " ('ta', 109),\n",
       " ('thanks', 109),\n",
       " ('isnt', 108),\n",
       " ('id', 108),\n",
       " ('doesnt', 107),\n",
       " ('trying', 107),\n",
       " ('made', 107),\n",
       " ('wow', 103),\n",
       " ('money', 102),\n",
       " ('baby', 102),\n",
       " ('last', 101),\n",
       " ('family', 101),\n",
       " ('place', 101),\n",
       " ('told', 101),\n",
       " ('cool', 100),\n",
       " ('guess', 99),\n",
       " ('years', 99),\n",
       " ('old', 98),\n",
       " ('ask', 98),\n",
       " ('hear', 98),\n",
       " ('listen', 98),\n",
       " ('name', 97),\n",
       " ('crazy', 96),\n",
       " ('dude', 96),\n",
       " ('wanted', 95),\n",
       " ('believe', 94),\n",
       " ('wasnt', 93),\n",
       " ('hell', 93),\n",
       " ('real', 92),\n",
       " ('house', 92),\n",
       " ('nothing', 91),\n",
       " ('whos', 90),\n",
       " ('whole', 89),\n",
       " ('away', 89),\n",
       " ('done', 89),\n",
       " ('next', 89),\n",
       " ('tonight', 88),\n",
       " ('black', 88),\n",
       " ('second', 87),\n",
       " ('room', 87),\n",
       " ('friend', 86),\n",
       " ('find', 86),\n",
       " ('friends', 83),\n",
       " ('everything', 83),\n",
       " ('every', 83),\n",
       " ('date', 82),\n",
       " ('pretty', 80),\n",
       " ('ah', 79),\n",
       " ('huh', 79),\n",
       " ('three', 79),\n",
       " ('probably', 78),\n",
       " ('stuff', 78),\n",
       " ('wrong', 77),\n",
       " ('son', 76),\n",
       " ('honey', 76),\n",
       " ('today', 76),\n",
       " ('coming', 76),\n",
       " ('move', 74),\n",
       " ('idea', 73),\n",
       " ('wont', 73),\n",
       " ('happy', 72),\n",
       " ('long', 72),\n",
       " ('hard', 72),\n",
       " ('ready', 72),\n",
       " ('kid', 72),\n",
       " ('everybody', 72),\n",
       " ('world', 71),\n",
       " ('fun', 71),\n",
       " ('youll', 71),\n",
       " ('live', 71),\n",
       " ('mr', 71),\n",
       " ('saying', 70),\n",
       " ('happened', 70),\n",
       " ('women', 69),\n",
       " ('remember', 69),\n",
       " ('morning', 69),\n",
       " ('sex', 69),\n",
       " ('youve', 69),\n",
       " ('care', 69),\n",
       " ('also', 68),\n",
       " ('problem', 68),\n",
       " ('phone', 68),\n",
       " ('drink', 67),\n",
       " ('someone', 67),\n",
       " ('music', 67),\n",
       " ('excuse', 65),\n",
       " ('may', 65),\n",
       " ('door', 64),\n",
       " ('looking', 64)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a list of the FreqDist of our scripts list\n",
    "lines_freqdist = FreqDist(lines_concat)\n",
    "#print the most common 200 words in our scripts\n",
    "lines_freqdist.most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not too informative because we don't have the breakdown of class distributions here, but it's still interesting to see which words are used the most in general. Something that surprised me was that the word \"dad\" was actually more used than the word \"mom\"! It's also funny that the word \"guys\" is right above the word \"people\", I wonder if those words are used interchangeably a lot, or if failed sitcoms tend to use one term more than the other. To get the information we really want, let's use TF-IDF Vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vector'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To properly use scikit-learn's TfidfVectorizer, we must pass in the data as raw text documents because the TfidfVectorizer likes to handle the Count Vectorization process on it's own and then fit and transforms the data into TF-IDF format. Additionally, for the purposes of TF-IDF vectorization, we do not want to clean or \"omit\" anything from our data (such as stop words or common words) because we will be losing information.\n",
    " \n",
    "Don't worry though, it was still necessary to pre-process the data to get our total vocabulary variable above and we will use our cleaned dataframe when we work on deep learning in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting our target from our feature set\n",
    "#normally, we would do an 80/20 split, but because of how small our data is we want to expand the test set as much as possible\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "    data_text.script, \n",
    "    data_text.label,\n",
    "    train_size=0.75, \n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#initialize vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#fit it to test and train sets\n",
    "tf_idf_data_train = vectorizer.fit_transform(X_train)\n",
    "tf_idf_data_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21362, 8771)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_data_train.shape #check the size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vectorized data contains 21025 articles, with 8812 unique words in the vocabulary. \n",
    "\n",
    "Since each line in our scripts only uses a tiny subset of our total vocabulary, the majority of our columns for any given line of script will be zero. These extremely zero-heavy vectors are called \"Sparse Vectors\" and are very common in NLP projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Articles: 5.308959835221422\n",
      "Percentage of columns containing 0: 0.9993947144185131\n"
     ]
    }
   ],
   "source": [
    "#get average of non-zero elements\n",
    "non_zero_cols = tf_idf_data_train.nnz / float(tf_idf_data_train.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "#calculate percentage\n",
    "percent_sparse = 1 - (non_zero_cols / float(tf_idf_data_train.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='class'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start by running six classifiers on on our TF-IDF data: \n",
    "\n",
    "Multinomial Naive Bayes, Random Forest, Logistic Regression, Decision Tree, K-Nearest Neighbors, and Support Vector Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the classifiers\n",
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "lr_classifier = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "svc_classifier = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emily\\Anaconda3\\Newfolder\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit support vector machine classifier and predict on train and test data\n",
    "svc_classifier.fit(tf_idf_data_train, y_train)\n",
    "svc_train_preds = svc_classifier.predict(tf_idf_data_train)\n",
    "svc_test_preds = svc_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit k-nearest neighbors classifier and predict on train and test data\n",
    "knn_classifier.fit(tf_idf_data_train, y_train)\n",
    "knn_train_preds = knn_classifier.predict(tf_idf_data_train)\n",
    "knn_test_preds = knn_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit decision tree classifier and predict on test and train data\n",
    "dt_classifier.fit(tf_idf_data_train, y_train)\n",
    "dt_train_preds = dt_classifier.predict(tf_idf_data_train)\n",
    "dt_test_preds = dt_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit logistic regression classifier and predict on test and train data\n",
    "lr_classifier.fit(tf_idf_data_train, y_train)\n",
    "lr_train_preds = lr_classifier.predict(tf_idf_data_train)\n",
    "lr_test_preds = lr_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "#fit multinomial naive bayes classifier and predict on test and train data\n",
    "nb_classifier.fit(tf_idf_data_train, y_train)\n",
    "nb_train_preds = nb_classifier.predict(tf_idf_data_train)\n",
    "nb_test_preds = nb_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit random forest classifier and predict on test and train data\n",
    "rf_classifier.fit(tf_idf_data_train, y_train)\n",
    "rf_train_preds = rf_classifier.predict(tf_idf_data_train)\n",
    "rf_test_preds = rf_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see how each of these classifiers performed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tfperm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. TF-IDF Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes\n",
      "Training Accuracy: 0.7454 \t\t Testing Accuracy: 0.6249\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Random Forest\n",
      "Training Accuracy: 0.9256 \t\t Testing Accuracy: 0.5855\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Logistic Regression\n",
      "Training Accuracy: 0.7346 \t\t Testing Accuracy: 0.6192\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Decision Tree\n",
      "Training Accuracy: 0.9256 \t\t Testing Accuracy: 0.5607\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "K-Nearest Neighbors\n",
      "Training Accuracy: 0.6505 \t\t Testing Accuracy: 0.5498\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Support Vector Machine\n",
      "Training Accuracy: 0.5253 \t\t Testing Accuracy: 0.527\n"
     ]
    }
   ],
   "source": [
    "#compute accuracy scores on both test set and train set for each classifier\n",
    "nb_train_score = accuracy_score(y_train, nb_train_preds)\n",
    "nb_test_score = accuracy_score(y_test, nb_test_preds)\n",
    "rf_train_score = accuracy_score(y_train, rf_train_preds)\n",
    "rf_test_score = accuracy_score(y_test, rf_test_preds)\n",
    "lr_train_score = accuracy_score(y_train, lr_train_preds)\n",
    "lr_test_score = accuracy_score(y_test, lr_test_preds)\n",
    "dt_train_score = accuracy_score(y_train, dt_train_preds)\n",
    "dt_test_score = accuracy_score(y_test, dt_test_preds)\n",
    "knn_train_score = accuracy_score(y_train, knn_train_preds)\n",
    "knn_test_score = accuracy_score(y_test, knn_test_preds)\n",
    "svc_train_score = accuracy_score(y_train, svc_train_preds)\n",
    "svc_test_score = accuracy_score(y_test, svc_test_preds)\n",
    "\n",
    "\n",
    "#print test/train accuracy scores for each classifier\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(nb_train_score, nb_test_score))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Random Forest')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score, rf_test_score))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Logistic Regression')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(lr_train_score, lr_test_score))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Decision Tree')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(dt_train_score, dt_test_score))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('K-Nearest Neighbors')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(knn_train_score, knn_test_score))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Support Vector Machine')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(svc_train_score, svc_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best classifier on the testing data was Multinomial Naive Bayes, but our best classifier on the training data was actually a tie between Random Forest and Decision Tree. Overall, it looks like the classifiers performed much better on the training set than they did on the test set. This means that we have some over-fitting problems with our data, which we may be able to solve through tuning. However, because of the small size of the dataset, there might not be much that we can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lsa'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Semantic Analysis (LSA) is used when we want to perform dimensionality reduction on text data. As we saw from the shape of tf_idf_data_train, our vectors are long and unwieldy because each word in our total vocabulary has its own component. To make this data more manageable, we can use SKlearn'sfunction called \"TruncatedSVD\" which performs dimensionality reduction. Not only will LSA reduce computational load, it improves the type of features that we keep. In other words, it trades a large number of mixed-performance features for a smaller set of better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize SVD and then apply SVD to test and train tf/idf sets\n",
    "svd = TruncatedSVD(n_components=100, random_state=42) #dimensionality of output data, for LSA a value of 100 is recommended\n",
    "lsa_train = svd.fit_transform(tf_idf_data_train) \n",
    "lsa_test = svd.fit_transform(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to go ahead and run the same classifiers that we used above, but this time we will omit Multinomial Naive Bayes, because it does not usually work with SVD or other matrix factorization. Additionally, MNB will not run on negative values, so we would have to use NMP (a non-negative matrix factorization) instead of SVD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emily\\Anaconda3\\Newfolder\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit support vector machine classifier and predict on train and test data\n",
    "svc_classifier.fit(lsa_train, y_train)\n",
    "svc_train_preds = svc_classifier.predict(lsa_train)\n",
    "svc_test_preds = svc_classifier.predict(lsa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit k-nearest neighbors classifier and predict on train and test data\n",
    "knn_classifier.fit(lsa_train, y_train)\n",
    "knn_train_preds = knn_classifier.predict(lsa_train)\n",
    "knn_test_preds = knn_classifier.predict(lsa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit decision tree classifier and predict on test and train data\n",
    "dt_classifier.fit(lsa_train, y_train)\n",
    "dt_train_preds = dt_classifier.predict(lsa_train)\n",
    "dt_test_preds = dt_classifier.predict(lsa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit logistic regression classifier and predict on test and train data\n",
    "lr_classifier.fit(lsa_train, y_train)\n",
    "lr_train_preds = lr_classifier.predict(lsa_train)\n",
    "lr_test_preds = lr_classifier.predict(lsa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit random forest classifier and predict on test and train data\n",
    "rf_classifier.fit(lsa_train, y_train)\n",
    "rf_train_preds = rf_classifier.predict(lsa_train)\n",
    "rf_test_preds = rf_classifier.predict(lsa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lsaperm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.  LSA Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Training Accuracy: 0.9203 \t\t Testing Accuracy: 0.5061\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Logistic Regression\n",
      "Training Accuracy: 0.5453 \t\t Testing Accuracy: 0.5165\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Decision Tree\n",
      "Training Accuracy: 0.9207 \t\t Testing Accuracy: 0.5088\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "K-Nearest Neighbors\n",
      "Training Accuracy: 0.6763 \t\t Testing Accuracy: 0.505\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Support Vector Machine\n",
      "Training Accuracy: 0.5253 \t\t Testing Accuracy: 0.527\n"
     ]
    }
   ],
   "source": [
    "#compute accuracy scores on both test set and train set for each classifier\n",
    "\n",
    "rf_train_score2 = accuracy_score(y_train, rf_train_preds)\n",
    "rf_test_score2 = accuracy_score(y_test, rf_test_preds)\n",
    "lr_train_score2 = accuracy_score(y_train, lr_train_preds)\n",
    "lr_test_score2 = accuracy_score(y_test, lr_test_preds)\n",
    "dt_train_score2 = accuracy_score(y_train, dt_train_preds)\n",
    "dt_test_score2 = accuracy_score(y_test, dt_test_preds)\n",
    "knn_train_score2 = accuracy_score(y_train, knn_train_preds)\n",
    "knn_test_score2 = accuracy_score(y_test, knn_test_preds)\n",
    "svc_train_score2 = accuracy_score(y_train, svc_train_preds)\n",
    "svc_test_score2 = accuracy_score(y_test, svc_test_preds)\n",
    "\n",
    "\n",
    "#print test/train accuracy scores for each classifier\n",
    "\n",
    "print('Random Forest')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score2, rf_test_score2))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Logistic Regression')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(lr_train_score2, lr_test_score2))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Decision Tree')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(dt_train_score2, dt_test_score2))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('K-Nearest Neighbors')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(knn_train_score2, knn_test_score2))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Support Vector Machine')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(svc_train_score2, svc_test_score2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test scores look pretty much the same, if not slightly lower than they were when we modeled on the TF-IDF data without LSA. Meanwhile, all of our test scores are hovering around 50%, which is definitely a negative change. Once again, we have much higher scores on our training set than we do on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='combo'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Combining TF-IDF with Our New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, now that we've explored TF-IDF a bit, let's see what the features that we created are capable of! Let's combine our text-descriptive features with our TF-IDF data and then run a few of our classifiers again.\n",
    "\n",
    "First things first, though - before we can model our data, we will standardize it using sklearn's \"Standard Scaler\". Then we will perform our test train split on our new, standardized data numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we need to perform TF-IDF vectorization again, we will re-open our uncleaned df that still has our features included\n",
    "scaled_data = pd.read_csv('pilotfeatures.csv') #open and read features script df again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scale'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Scaling Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of columns that we want normalized\n",
    "num_cols = ['numerics','upper', 'word_count', 'num_exclamation_marks', 'num_question_marks',\n",
    "       'num_punctuation', 'avg_word', 'contro', 'names', 'nouns', 'adjectives',\n",
    "       'verbs', 'nouns_vs_words', 'adjectives_vs_words', 'verbs_vs_words']\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#standardize our numerical features\n",
    "scaled_data[num_cols] = StandardScaler().fit_transform(scaled_data[num_cols])\n",
    "\n",
    "#splitting our target from our feature set\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "    scaled_data, \n",
    "    scaled_data.label,\n",
    "    train_size=0.75, \n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run tf-idf vectorization on train and test sets again\n",
    "tf_idf_data_train = vectorizer.fit_transform(X_train.script)\n",
    "tf_idf_data_test = vectorizer.transform(X_test.script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we have a few other columns tucked into our dataset, such as \"title\" and our handy-dandy script variable. When setting X_train features, we will make sure to only include our text descriptive features in our set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X_train/X_test features so that we can add them to our tf-idf data\n",
    "X_train_features = X_train[num_cols] \n",
    "X_test_features = X_test[num_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='combine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. SciPy Hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we our TF-IDF data and feature data are both ready to go, we can combine them using SciPy's hstack function, which is used to horizontally combine sparse matrices without having to convert anything to a dense format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "#combine our tf-idf data with self-made features\n",
    "X_train = hstack([tf_idf_data_train, X_train_features])\n",
    "X_test = hstack([tf_idf_data_test, X_test_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21362, 8786)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape #check shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our previous X-train shape (when it only included our TF-IDF data) was (21362, 8771). Our current X-train shape is now (21362, 8786) to account for the 15 descriptive features that we added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit k-nearest neighbors classifier and predict on train and test data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "knn_train_preds = knn_classifier.predict(X_train)\n",
    "knn_test_preds = knn_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit decision tree classifier and predict on test and train data\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "dt_train_preds = dt_classifier.predict(X_train)\n",
    "dt_test_preds = dt_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 672 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit logistic regression classifier and predict on test and train data\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "lr_train_preds = lr_classifier.predict(X_train)\n",
    "lr_test_preds = lr_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit random forest classifier and predict on test and train data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_train_preds = rf_classifier.predict(X_train)\n",
    "rf_test_preds = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comboperm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. TF-IDF/Feature Combination Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fingers crossed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Training Accuracy: 0.9433 \t\t Testing Accuracy: 0.5828\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Logistic Regression\n",
      "Training Accuracy: 0.7351 \t\t Testing Accuracy: 0.6141\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Decision Tree\n",
      "Training Accuracy: 0.9433 \t\t Testing Accuracy: 0.5475\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "K-Nearest Neighbors\n",
      "Training Accuracy: 0.6865 \t\t Testing Accuracy: 0.5091\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_train_score3 = accuracy_score(y_train, rf_train_preds)\n",
    "rf_test_score3 = accuracy_score(y_test, rf_test_preds)\n",
    "lr_train_score3 = accuracy_score(y_train, lr_train_preds)\n",
    "lr_test_score3 = accuracy_score(y_test, lr_test_preds)\n",
    "dt_train_score3 = accuracy_score(y_train, dt_train_preds)\n",
    "dt_test_score3 = accuracy_score(y_test, dt_test_preds)\n",
    "knn_train_score3 = accuracy_score(y_train, knn_train_preds)\n",
    "knn_test_score3 = accuracy_score(y_test, knn_test_preds)\n",
    "\n",
    "print('Random Forest')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score3, rf_test_score3))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Logistic Regression')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(lr_train_score3, lr_test_score3))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Decision Tree')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(dt_train_score3, dt_test_score3))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('K-Nearest Neighbors')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(knn_train_score3, knn_test_score3))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy scores for our TF-IDF/feature combination data are better than our LSA data, but they do not look much different than how our TF-IDF data performed alone. In fact, each of the scores seem to be a bit lower than our lone TF-IDF data. That seems to suggest that the features that we generated for our data do not perform very well, but I think it's still worth it to run a few classifiers on only our feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Classification on Descriptive Feature Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how our core four classifiers perform when just using the descriptive features that we generated. I do not have much hope for this run-through, but I would like to know for sure how their performance ranks. The only preparation that we will be doing that we haven't done yet is one-hot encoding our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#initialize label encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "#one-hot encode our label column\n",
    "scaled_data['label'] = le.fit_transform(scaled_data['label'])\n",
    "\n",
    "#normalize our numerical features\n",
    "scaled_data[num_cols] = StandardScaler().fit_transform(scaled_data[num_cols])\n",
    "\n",
    "#splitting our target variable from our train set one last time (at least in this notebook)\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "    scaled_data[num_cols], \n",
    "    scaled_data.label,\n",
    "    train_size=0.75, \n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit k-nearest neighbors classifier and predict on train and test data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "knn_train_preds = knn_classifier.predict(X_train)\n",
    "knn_test_preds = knn_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit decision tree classifier and predict on test and train data\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "dt_train_preds = dt_classifier.predict(X_train)\n",
    "dt_test_preds = dt_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 86 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit logistic regression classifier and predict on test and train data\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "lr_train_preds = lr_classifier.predict(X_train)\n",
    "lr_test_preds = lr_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit random forest classifier and predict on test and train data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_train_preds = rf_classifier.predict(X_train)\n",
    "rf_test_preds = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='featperm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Descriptive Feature Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Training Accuracy: 0.808 \t\t Testing Accuracy: 0.5036\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Logistic Regression\n",
      "Training Accuracy: 0.53 \t\t Testing Accuracy: 0.5276\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Decision Tree\n",
      "Training Accuracy: 0.808 \t\t Testing Accuracy: 0.5046\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "K-Nearest Neighbors\n",
      "Training Accuracy: 0.6533 \t\t Testing Accuracy: 0.504\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_train_score4 = accuracy_score(y_train, rf_train_preds)\n",
    "rf_test_score4 = accuracy_score(y_test, rf_test_preds)\n",
    "lr_train_score4 = accuracy_score(y_train, lr_train_preds)\n",
    "lr_test_score4 = accuracy_score(y_test, lr_test_preds)\n",
    "dt_train_score4 = accuracy_score(y_train, dt_train_preds)\n",
    "dt_test_score4 = accuracy_score(y_test, dt_test_preds)\n",
    "knn_train_score4 = accuracy_score(y_train, knn_train_preds)\n",
    "knn_test_score4 = accuracy_score(y_test, knn_test_preds)\n",
    "\n",
    "print('Random Forest')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score4, rf_test_score4))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Logistic Regression')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(lr_train_score4, lr_test_score4))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Decision Tree')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(dt_train_score4, dt_test_score4))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('K-Nearest Neighbors')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(knn_train_score4, knn_test_score4))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, the descriptive feature set did not do too well by itself. The test set scores for all of the classifiers hover around 50%, with the high being Logistic Regression with 52%. Additionally, our data is still suffering from over-fitting, as evident from the much higher scores on the training set vs the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluation of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile all of our scores into one data frame so that we can easily compare everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe for TF-IDF\n",
    "classifiers1 = [\"Multinomial Naive Bayes\", \"Random Forest\", \"Logistic Regression\", \"Decision Tree\",\n",
    "                \"K-Nearest Neighbors\", \"Support Vector Machine\"] \n",
    "train1 = [nb_train_score, rf_train_score, lr_train_score, dt_train_score, knn_train_score, svc_train_score]\n",
    "test1 = [nb_test_score, rf_test_score, lr_test_score, dt_test_score, knn_test_score, svc_test_score]\n",
    "labels1 = [\"TF-IDF\", \"TF-IDF\", \"TF-IDF\", \"TF-IDF\", \"TF-IDF\", \"TF-IDF\"]\n",
    "\n",
    "df1 = pd.DataFrame(list(zip(classifiers1, labels1, train1, test1)),\n",
    "                   columns=['Classifier','Data', 'Train Score', 'Test Score'])\n",
    "\n",
    "#create dataframe for TF-IDF with LSA\n",
    "classifiers2 = [ \"Random Forest\", \"Logistic Regression\", \"Decision Tree\",\n",
    "                \"K-Nearest Neighbors\", \"Support Vector Machine\"]\n",
    "train2 = [rf_train_score2, lr_train_score2, dt_train_score2, knn_train_score2, svc_train_score2]\n",
    "test2 =  [rf_test_score2, lr_test_score2, dt_test_score2, knn_test_score2, svc_test_score2]\n",
    "labels2 = [\"LSA\", \"LSA\", \"LSA\", \"LSA\", \"LSA\"]\n",
    "\n",
    "df2 = pd.DataFrame(list(zip(classifiers2, labels2, train2, test2)),\n",
    "                   columns=['Classifier','Data', 'Train Score', 'Test Score'])\n",
    "\n",
    "#create dataframe for combination of TF-IDF and Features\n",
    "classifiers3 = [ \"Random Forest\", \"Logistic Regression\", \"Decision Tree\",\n",
    "                \"K-Nearest Neighbors\"]\n",
    "train3 = [rf_train_score3, lr_train_score3, dt_train_score3, knn_train_score3]\n",
    "test3 =  [rf_test_score3, lr_test_score3, dt_test_score3, knn_test_score3]\n",
    "labels3 = [\"Combo\", \"Combo\", \"Combo\", \"Combo\"]\n",
    "\n",
    "df3 = pd.DataFrame(list(zip(classifiers3, labels3, train3, test3)),\n",
    "                   columns=['Classifier','Data', 'Train Score', 'Test Score'])\n",
    "\n",
    "#create dataframe for Features\n",
    "classifiers4 = [\"Random Forest\", \"Logistic Regression\", \"Decision Tree\",\n",
    "                \"K-Nearest Neighbors\"]\n",
    "train4 = [rf_train_score4, lr_train_score4, dt_train_score4, knn_train_score4]\n",
    "test4 =  [rf_test_score4, lr_test_score4, dt_test_score4, knn_test_score4]\n",
    "labels4 = [\"Features\", \"Features\", \"Features\", \"Features\"]\n",
    "\n",
    "df4 = pd.DataFrame(list(zip(classifiers4, labels4, train4, test4)),\n",
    "                   columns=['Classifier','Data', 'Train Score', 'Test Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Test Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the best test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Data</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.745389</td>\n",
       "      <td>0.624912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.734622</td>\n",
       "      <td>0.619155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Combo</td>\n",
       "      <td>0.735137</td>\n",
       "      <td>0.614099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.925616</td>\n",
       "      <td>0.585451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Combo</td>\n",
       "      <td>0.943311</td>\n",
       "      <td>0.582783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.925616</td>\n",
       "      <td>0.560736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.650501</td>\n",
       "      <td>0.549782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Combo</td>\n",
       "      <td>0.943311</td>\n",
       "      <td>0.547535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Features</td>\n",
       "      <td>0.529960</td>\n",
       "      <td>0.527594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.525325</td>\n",
       "      <td>0.527033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.525325</td>\n",
       "      <td>0.527033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.545314</td>\n",
       "      <td>0.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>Combo</td>\n",
       "      <td>0.686499</td>\n",
       "      <td>0.509058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.920700</td>\n",
       "      <td>0.508777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.920326</td>\n",
       "      <td>0.506109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.676294</td>\n",
       "      <td>0.504985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Features</td>\n",
       "      <td>0.808024</td>\n",
       "      <td>0.504564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>Features</td>\n",
       "      <td>0.653263</td>\n",
       "      <td>0.504002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Features</td>\n",
       "      <td>0.808024</td>\n",
       "      <td>0.503581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Classifier      Data  Train Score  Test Score\n",
       "0  Multinomial Naive Bayes    TF-IDF     0.745389    0.624912\n",
       "2      Logistic Regression    TF-IDF     0.734622    0.619155\n",
       "1      Logistic Regression     Combo     0.735137    0.614099\n",
       "1            Random Forest    TF-IDF     0.925616    0.585451\n",
       "0            Random Forest     Combo     0.943311    0.582783\n",
       "3            Decision Tree    TF-IDF     0.925616    0.560736\n",
       "4      K-Nearest Neighbors    TF-IDF     0.650501    0.549782\n",
       "2            Decision Tree     Combo     0.943311    0.547535\n",
       "1      Logistic Regression  Features     0.529960    0.527594\n",
       "5   Support Vector Machine    TF-IDF     0.525325    0.527033\n",
       "4   Support Vector Machine       LSA     0.525325    0.527033\n",
       "1      Logistic Regression       LSA     0.545314    0.516500\n",
       "3      K-Nearest Neighbors     Combo     0.686499    0.509058\n",
       "2            Decision Tree       LSA     0.920700    0.508777\n",
       "0            Random Forest       LSA     0.920326    0.506109\n",
       "3      K-Nearest Neighbors       LSA     0.676294    0.504985\n",
       "2            Decision Tree  Features     0.808024    0.504564\n",
       "3      K-Nearest Neighbors  Features     0.653263    0.504002\n",
       "0            Random Forest  Features     0.808024    0.503581"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval =pd.concat([df1, df2, df3, df4], axis=0) #concatenate dataframes of scores\n",
    "df_eval.sort_values(by=['Test Score'], ascending = False) #sort by test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our top performing classifier was actually the first classifier that we ran - Multinomial Naive Bayes on TF-IDF! As for the data, TF-IDF by itself easily had the highest scores. The next dataset that was most successful is the combination dataframe, which has 2 of the 5 top scores. The Features alone dataset had the bottom three test scores, with the lowest being .5035. However, that doesn't say too much seeing as 7 of the 19 models have values of about 50%! Let's move on to looking at the most successful training scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Train Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Data</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Combo</td>\n",
       "      <td>0.943311</td>\n",
       "      <td>0.547535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Combo</td>\n",
       "      <td>0.943311</td>\n",
       "      <td>0.582783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.925616</td>\n",
       "      <td>0.560736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.925616</td>\n",
       "      <td>0.585451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.920700</td>\n",
       "      <td>0.508777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.920326</td>\n",
       "      <td>0.506109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Features</td>\n",
       "      <td>0.808024</td>\n",
       "      <td>0.504564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Features</td>\n",
       "      <td>0.808024</td>\n",
       "      <td>0.503581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.745389</td>\n",
       "      <td>0.624912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Combo</td>\n",
       "      <td>0.735137</td>\n",
       "      <td>0.614099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.734622</td>\n",
       "      <td>0.619155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>Combo</td>\n",
       "      <td>0.686499</td>\n",
       "      <td>0.509058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.676294</td>\n",
       "      <td>0.504985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>Features</td>\n",
       "      <td>0.653263</td>\n",
       "      <td>0.504002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.650501</td>\n",
       "      <td>0.549782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.545314</td>\n",
       "      <td>0.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Features</td>\n",
       "      <td>0.529960</td>\n",
       "      <td>0.527594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.525325</td>\n",
       "      <td>0.527033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.525325</td>\n",
       "      <td>0.527033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Classifier      Data  Train Score  Test Score\n",
       "2            Decision Tree     Combo     0.943311    0.547535\n",
       "0            Random Forest     Combo     0.943311    0.582783\n",
       "3            Decision Tree    TF-IDF     0.925616    0.560736\n",
       "1            Random Forest    TF-IDF     0.925616    0.585451\n",
       "2            Decision Tree       LSA     0.920700    0.508777\n",
       "0            Random Forest       LSA     0.920326    0.506109\n",
       "2            Decision Tree  Features     0.808024    0.504564\n",
       "0            Random Forest  Features     0.808024    0.503581\n",
       "0  Multinomial Naive Bayes    TF-IDF     0.745389    0.624912\n",
       "1      Logistic Regression     Combo     0.735137    0.614099\n",
       "2      Logistic Regression    TF-IDF     0.734622    0.619155\n",
       "3      K-Nearest Neighbors     Combo     0.686499    0.509058\n",
       "3      K-Nearest Neighbors       LSA     0.676294    0.504985\n",
       "3      K-Nearest Neighbors  Features     0.653263    0.504002\n",
       "4      K-Nearest Neighbors    TF-IDF     0.650501    0.549782\n",
       "1      Logistic Regression       LSA     0.545314    0.516500\n",
       "1      Logistic Regression  Features     0.529960    0.527594\n",
       "4   Support Vector Machine       LSA     0.525325    0.527033\n",
       "5   Support Vector Machine    TF-IDF     0.525325    0.527033"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.sort_values(by=['Train Score'], ascending = False) #sort by Train score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting that when you sort the data by training score, our dataframe looks a lot differently than it did before. Our top performing model was tied between Decision Tree on the combination data and Random Forest on the combination data, both with values of .9433. There also isn't as much of a clear lead between the datasets we used, however there is a pattern for which classifiers are most successful. Decision Tree and Random Forest have nearly identical scores on the train sets and make up the top 8 scores. Our lowest score is Support Vector Machine on the TF-IDF data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that wraps up our classification notebook. Next up is deep learning! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
